{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmiV3MZDu6q0ztTyY4rQKy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "781f32fafc4a425f97c1904a36b67c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_322354e5b44d449ba532d974b2956d04",
              "IPY_MODEL_ea23059236c44fcd8ec80021fe6ff760",
              "IPY_MODEL_9380b4cbffcd4ec187aee73a14db3d26"
            ],
            "layout": "IPY_MODEL_2bcdcb70250b4dd498e24c6cfb598ee6"
          }
        },
        "322354e5b44d449ba532d974b2956d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca1143e2b5d43f7a6a93aa3b1f4ace9",
            "placeholder": "​",
            "style": "IPY_MODEL_94cad8d5ddd845718654532773c4f8ba",
            "value": "Downloading builder script: 100%"
          }
        },
        "ea23059236c44fcd8ec80021fe6ff760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c8fed18708420aaabf8e5f8910b24a",
            "max": 8146,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47919d1b7a1645f6b7a5e0d1437cfd5c",
            "value": 8146
          }
        },
        "9380b4cbffcd4ec187aee73a14db3d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e46662c80acd4351954a5888a332e1d0",
            "placeholder": "​",
            "style": "IPY_MODEL_4bd7948ae7014036b1f3bd3ca8ff9fd1",
            "value": " 8.15k/8.15k [00:00&lt;00:00, 528kB/s]"
          }
        },
        "2bcdcb70250b4dd498e24c6cfb598ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca1143e2b5d43f7a6a93aa3b1f4ace9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94cad8d5ddd845718654532773c4f8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75c8fed18708420aaabf8e5f8910b24a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47919d1b7a1645f6b7a5e0d1437cfd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e46662c80acd4351954a5888a332e1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd7948ae7014036b1f3bd3ca8ff9fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-chim/QMUL-Thesis-Draft/blob/main/cl_synthetic_data_evaluation_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "This notebook contains example intrinsic evaluation code described in our paper: [Evaluating Synthetic Data Generation from User Generated Text (Chim et al., CL 2024)](https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00540/124625).\n",
        "\n",
        "## Setup\n",
        "The notebook is split by evaluation aspect - meaning, style, divergence.\n",
        "\n",
        "Most sections can be directly ran with little setup. However, for style evaluation, you will need to use the idiolect model trained by [Zhu and Jurgen, 2021](https://aclanthology.org/2021.emnlp-main.25/) or obtain alternative style-sensitive embeddings. Our paper uses the following weights, re-saved from Zhu and Jurgen's model for software version compatibility: https://drive.google.com/file/d/1SXSlp4K9sM5EOhiwkP-XjkUZIzc9worB/view?usp=sharing. Ensure you save this in your drive (if running directly on colab) or download it for offline use.\n",
        "\n"
      ],
      "metadata": {
        "id": "-ImP4s68WNvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example synthetic texts from a single source, varying in style and meaning similarity\n",
        "\n",
        "synthetic_texts = [\n",
        "    \"The nimble brown fox hops across the sleepy dog.\",\n",
        "    \"the lazy canine was lying around when it got jumped over by a quick-moving brown fox!\",\n",
        "    \"despite heavy rain yesterday evening, remember to water those plants!\"\n",
        "]\n",
        "\n",
        "original_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\"\n",
        "] * len(synthetic_texts)"
      ],
      "metadata": {
        "id": "TjJ01_EUbJ4A"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meaning"
      ],
      "metadata": {
        "id": "fxfX_6nDZMu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the main reported metric is BERTScore, which is conveniently run using the official implementation\n",
        "%%capture\n",
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "hPHj5bvaZL6t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "\n",
        "scorer = BERTScorer(model_type=\"roberta-large\", lang=\"en\")\n",
        "# we report the mean (F.mean()) in our paper\n",
        "_, _, F = scorer.score(synthetic_texts, original_texts)\n",
        "\n",
        "print(\"\\nBERTScore of each example text:\")\n",
        "for score, synthetic_text in zip(F, synthetic_texts):\n",
        "    print(f\"{synthetic_text} (score: {score.item():.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy59IZPka_tW",
        "outputId": "5b10547e-4b11-4797-ed2c-731534d778a6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERTScore of each example text:\n",
            "The nimble brown fox hops across the sleepy dog. (score: 0.96)\n",
            "the lazy canine was lying around when it got jumped over by a quick-moving brown fox! (score: 0.91)\n",
            "despite heavy rain yesterday evening, remember to water those plants! (score: 0.84)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Style"
      ],
      "metadata": {
        "id": "sC5bux3WZOd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding-based"
      ],
      "metadata": {
        "id": "3g5ZDwjRdW0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers==4.30.2 # needed to load style embeddings"
      ],
      "metadata": {
        "id": "QL2DuG2xdJNp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaConfig, RobertaModel, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Adapted from: https://github.com/lingjzhu/idiolect\n",
        "\n",
        "class AttentionPooling(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of SelfAttentionPooling\n",
        "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
        "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.W = torch.nn.Linear(input_dim, 1)\n",
        "        self.softmax = torch.nn.functional.softmax\n",
        "\n",
        "    def forward(self, batch_rep, att_mask=None):\n",
        "        \"\"\"\n",
        "        N: batch size, T: sequence length, H: Hidden dimension\n",
        "        input:\n",
        "            batch_rep : size (N, T, H)\n",
        "        attention_weight:\n",
        "            att_w : size (N, T, 1)\n",
        "        return:\n",
        "            utter_rep: size (N, H)\n",
        "        \"\"\"\n",
        "        att_logits = self.W(batch_rep).squeeze(-1)\n",
        "        if att_mask is not None:\n",
        "            att_logits = att_mask + att_logits\n",
        "        att_w = self.softmax(att_logits, dim=-1).unsqueeze(-1)\n",
        "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
        "\n",
        "        return utter_rep\n",
        "\n",
        "\n",
        "class DNNSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, **kwargs):\n",
        "        super(DNNSelfAttention, self).__init__()\n",
        "        self.pooling = AttentionPooling(hidden_dim)\n",
        "        self.out_layer = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, features, att_mask):\n",
        "        out = self.pooling(features, att_mask).squeeze(-1)\n",
        "        predicted = self.out_layer(out)\n",
        "        return predicted\n",
        "\n",
        "\n",
        "class SRoberta(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"roberta-base\"):\n",
        "        super().__init__()\n",
        "        config = RobertaConfig.from_pretrained(model_name, return_dict=True)\n",
        "        config.output_hidden_states = True\n",
        "        self.roberta = RobertaModel.from_pretrained(model_name, config=config)\n",
        "\n",
        "        self.pooler = DNNSelfAttention(768)\n",
        "\n",
        "    def forward(self, input_ids, att_mask=None):\n",
        "        out = self.roberta(input_ids, att_mask)\n",
        "        out = out.last_hidden_state\n",
        "        out = self.pooler(out, att_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def batch_embed(texts, model, tokenizer, max_length=512):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "    with torch.no_grad():\n",
        "        hidden = model(\n",
        "            inputs['input_ids'].to(device),\n",
        "            inputs['attention_mask'].to(device)\n",
        "        )\n",
        "        hidden = F.normalize(hidden, dim=-1).cpu().detach()\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "kGtusIeRyQF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load model\n",
        "# The following implementation assumes you are loading directly from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive/\")\n",
        "\n",
        "style_model_path = \"/content/MyDrive/MyDrive/experiments/sroberta_model-4_reddit_resave.bin\" # replace with your path\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = SRoberta()\n",
        "if torch.cuda.is_available():\n",
        "    model.load_state_dict(torch.load(style_model_path))\n",
        "else:\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            style_model_path,\n",
        "            map_location=torch.device('cpu')\n",
        "        )\n",
        "    )\n",
        "_ = model.to(device)\n",
        "\n",
        "# 2. Extract embeddings\n",
        "original_embeds = batch_embed(original_texts, model, tokenizer)\n",
        "synthetic_embeds = batch_embed(synthetic_texts, model, tokenizer)\n",
        "\n",
        "# 3. Compute embedding similarity\n",
        "scores = np.array([1 - cosine(a, b) for a, b in zip(original_embeds, synthetic_embeds)])\n",
        "# if all synthetic texts are from the same system, we can report scores.mean()\n",
        "print(\"\\nIdiolect embedding scores for each example text:\")\n",
        "for score, synthetic_text in zip(scores, synthetic_texts):\n",
        "    print(f\"{synthetic_text} (score: {score.item():.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeUZ2oWndlPX",
        "outputId": "e1708e94-578a-448d-957d-04d13fbbfdd4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive/; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-34-b13d1bd0cd4c>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Idiolect embedding scores for each example text:\n",
            "The nimble brown fox hops across the sleepy dog. (score: 0.90)\n",
            "the lazy canine was lying around when it got jumped over by a quick-moving brown fox! (score: 0.69)\n",
            "despite heavy rain yesterday evening, remember to water those plants! (score: 0.56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS-based"
      ],
      "metadata": {
        "id": "PE9uACYTdZom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class POSStyleSimilarityScorer:\n",
        "    def __init__(self):\n",
        "        # Ireland and Pennebaker, 2010 captures writing styles\n",
        "        # by examining POS tag occurences across categories:\n",
        "        # 0) adv, 1) adj, 2) conj, 3) det, 4) noun, 5) pron, 6) preposition, 7) punct\n",
        "        self._VALID_UPOS = {\n",
        "            \"ADV\",\n",
        "            \"ADJ\",\n",
        "            # NO AUX,\n",
        "            \"CCONJ\",\n",
        "            \"SCONJ\",\n",
        "            \"DET\",\n",
        "            # NO INTJ\n",
        "            \"NOUN\",\n",
        "            \"PROPN\",\n",
        "            # NO NUM\n",
        "            \"PRON\",\n",
        "            \"ADP\",\n",
        "            \"PART\",\n",
        "            \"PUNCT\",\n",
        "            # NO SYMB\n",
        "            # NO VERB,\n",
        "            # NO X\n",
        "        }\n",
        "        self.VALID_UPOS = sorted(self.map_tag(t) for t in self._VALID_UPOS)\n",
        "\n",
        "    def map_tag(self, tag):\n",
        "        # collapse UPOS tagset to categories\n",
        "        mapper = {\n",
        "            \"CCONJ\": \"CONJ\",\n",
        "            \"SCONJ\": \"CONJ\",\n",
        "            \"PROPN\": \"NOUN\",\n",
        "            \"ADP\": \"PREP\",\n",
        "            \"PART\": \"PREP\",\n",
        "            # BNC2014\n",
        "            \"SUBST\": \"NOUN\",\n",
        "            \"ART\": \"DET\",\n",
        "            \"INTERJ\": \"INTJ\",\n",
        "        }\n",
        "        return mapper.get(tag, tag)\n",
        "\n",
        "    def compute_jaccard_similarity(self, list1, list2):\n",
        "        set1, set2 = set(list1), set(list2)\n",
        "        intersection = list(set1.intersection(set2))\n",
        "        intersection_length = len(list(set1.intersection(set2)))\n",
        "        union_length = (len(set1) + len(set2)) - intersection_length\n",
        "        if union_length == 0:\n",
        "            return union_length\n",
        "        return float(intersection_length) / union_length\n",
        "\n",
        "    def tag_and_filter(self, text):\n",
        "        doc = nlp(text)\n",
        "        return [self.map_tag(t.pos_) for t in doc if t.pos_ in self._VALID_UPOS], len(\n",
        "            doc\n",
        "        )\n",
        "\n",
        "    def word_pos_score(self, pos1, pos2, len1, len2):\n",
        "        \"\"\"\n",
        "        Calculate POS similarity (Ireland and Pennbaker 2010) over UPOS tags.\n",
        "            1. for each POS category, get its count in proportion to total sentence length\n",
        "            2. calculate similarity score wrt each category\n",
        "            3. average to get total POS similarity score\n",
        "        \"\"\"\n",
        "        pos_counts1 = Counter(pos1)\n",
        "        pos_counts2 = Counter(pos2)\n",
        "\n",
        "        category_scores = []\n",
        "        for t in self.VALID_UPOS:\n",
        "            cat1 = pos_counts1.get(t, 0) / len1\n",
        "            cat2 = pos_counts2.get(t, 0) / len2\n",
        "            if cat1 == 0 and cat2 == 0:\n",
        "                score = 1\n",
        "            else:\n",
        "                score = 1 - (abs(cat1 - cat2) / (cat1 + cat2))\n",
        "            category_scores.append(score)\n",
        "\n",
        "        return np.mean(category_scores)\n",
        "\n",
        "    def trigram_pos_score(self, pos1, pos2):\n",
        "        # note that this will return 0 for shorter texts\n",
        "        pos1 = self._make_ngrams(pos1, n=3)\n",
        "        pos2 = self._make_ngrams(pos2, n=3)\n",
        "        return self.compute_jaccard_similarity(pos1, pos2)\n",
        "\n",
        "    def get_trigram_pos_score(self, text1, text2):\n",
        "        pos1, _ = self.tag_and_filter(text1)\n",
        "        pos2, _ = self.tag_and_filter(text2)\n",
        "        return self.trigram_pos_score(pos1, pos2)\n",
        "\n",
        "    def get_mean_trigram_pos_scores(self, src, targets, **kwargs):\n",
        "        return np.mean([self.get_trigram_pos_score(t, src) for t in targets]).item()\n",
        "\n",
        "    def _make_ngrams(self, l, n=3):\n",
        "        return [\"\".join(l[i : i + n]) for i in range(len(l) - n + 1)]\n",
        "\n",
        "scorer = POSStyleSimilarityScorer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_xsURVtwdGS",
        "outputId": "26debefe-084f-437d-ab0d-cb21653be74c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_pos_scores, word_pos_scores = [], []\n",
        "for orig_text, syn_text in zip(original_texts, synthetic_texts):\n",
        "    orig_pos, orig_len = scorer.tag_and_filter(orig_text)\n",
        "    syn_pos, syn_len = scorer.tag_and_filter(syn_text)\n",
        "    trigram_pos_score = scorer.get_trigram_pos_score(orig_text, syn_text)\n",
        "    word_pos_score = scorer.word_pos_score(orig_pos, syn_pos, orig_len, syn_len)\n",
        "    trigram_pos_scores.append(trigram_pos_score)\n",
        "    word_pos_scores.append(word_pos_score)\n",
        "\n",
        "print(\"\\nPOS-based scores for each example text:\")\n",
        "for i, synthetic_text in enumerate(synthetic_texts):\n",
        "    print(f\"{synthetic_text} (Word: {word_pos_scores[i]:.2f}; Trigram: {trigram_pos_scores[i]:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9epgp7TZQ_U",
        "outputId": "0d80543e-8025-4b56-9eb9-428adfa69f67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS-based scores for each example text:\n",
            "The nimble brown fox hops across the sleepy dog. (Word: 1.00; Trigram: 1.00)\n",
            "the lazy canine was lying around when it got jumped over by a quick-moving brown fox! (Word: 0.52; Trigram: 0.19)\n",
            "despite heavy rain yesterday evening, remember to water those plants! (Word: 0.64; Trigram: 0.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Divergence"
      ],
      "metadata": {
        "id": "bwx-HKOfZPWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aSZ7cq4WWBkz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "results = sacrebleu.compute(\n",
        "    predictions=synthetic_texts,\n",
        "    references=original_texts,\n",
        "    use_effective_order=True,\n",
        "    smooth_method='floor',\n",
        "    force=True\n",
        ") # use sentence-level\n",
        "\n",
        "# Aggregated BLEU between synthetic and original texts\n",
        "round(100 - results[\"score\"], 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "781f32fafc4a425f97c1904a36b67c82",
            "322354e5b44d449ba532d974b2956d04",
            "ea23059236c44fcd8ec80021fe6ff760",
            "9380b4cbffcd4ec187aee73a14db3d26",
            "2bcdcb70250b4dd498e24c6cfb598ee6",
            "4ca1143e2b5d43f7a6a93aa3b1f4ace9",
            "94cad8d5ddd845718654532773c4f8ba",
            "75c8fed18708420aaabf8e5f8910b24a",
            "47919d1b7a1645f6b7a5e0d1437cfd5c",
            "e46662c80acd4351954a5888a332e1d0",
            "4bd7948ae7014036b1f3bd3ca8ff9fd1"
          ]
        },
        "id": "0CeAYMecW0Oq",
        "outputId": "34d65566-19d5-4ef3-af2e-11e6539b4a80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:86: UserWarning: \n",
            "Access to the secret `HF_TOKEN` has not been granted on this notebook.\n",
            "You will not be requested again.\n",
            "Please restart the session if you want to be prompted again.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "781f32fafc4a425f97c1904a36b67c82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.628"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other\n",
        "\n",
        "## Distribution-level metrics"
      ],
      "metadata": {
        "id": "OMav9XkJZvAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: compare embedding distribution distances\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.stats import entropy\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Fréchet distance code adapted from: https://github.com/mchong6/FID_IS_infinity/blob/master/score_infinity.py\n",
        "\n",
        "def calculate_feature_statistics(feats):\n",
        "    \"\"\"Calculation of the statistics used by the FID.\n",
        "    Params:\n",
        "    -- feats       : tensor of features with the shape [N, D]\n",
        "    Returns:\n",
        "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
        "               the inception model.\n",
        "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
        "               the inception model.\n",
        "    \"\"\"\n",
        "    mu = np.mean(feats, axis=0) # (N, D)\n",
        "    sigma = np.cov(feats, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    Stable version by Dougal J. Sutherland.\n",
        "    Params:\n",
        "    -- mu1   : Numpy array containing the activations of a layer of the\n",
        "               inception net (like returned by the function 'get_predictions')\n",
        "               for generated samples.\n",
        "    -- mu2   : The sample mean over activations, precalculated on an\n",
        "               representative data set.\n",
        "    -- sigma1: The covariance matrix over activations for generated samples.\n",
        "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "               representative data set.\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = scipy.linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = scipy.linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1)\n",
        "            + np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "# Example usage - compare meaning distributions\n",
        "# (note this method is best used when there are larger numbers of examples)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = AutoModel.from_pretrained(\"roberta-large\")\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# if comparing style embedding distributions,\n",
        "# load the styleroberta (or other authorship-related models) instead\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "with torch.no_grad():\n",
        "    original_embeds = model(\n",
        "        **tokenizer(\n",
        "            original_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "            )\n",
        "        ).pooler_output\n",
        "    synthetic_embeds = model(\n",
        "        **tokenizer(\n",
        "            synthetic_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "            )\n",
        "        ).pooler_output\n",
        "    original_embeds = F.normalize(original_embeds, dim=-1).numpy()\n",
        "    synthetic_embeds = F.normalize(synthetic_embeds, dim=-1).numpy()\n",
        "\n",
        "orig_mu, orig_sigma = calculate_feature_statistics(original_embeds)\n",
        "syn_mu, syn_sigma = calculate_feature_statistics(synthetic_embeds)\n",
        "distance = calculate_frechet_distance(orig_mu, orig_sigma, syn_mu, syn_sigma, eps=1e-6)\n",
        "\n",
        "print(f\"\\nFréchet distance: {distance:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q75Sr5gpZv36",
        "outputId": "1792daea-0f9d-47cf-a05d-105b7ca7179e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fréchet distance: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: compare individual POS tags at distribution level\n",
        "import spacy\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "def calculate_js_divergence(data1, data2):\n",
        "    # Convert datasets into probability distributions\n",
        "    max_val = max(max(data1), max(data2)) + 1\n",
        "    prob_dist1 = np.zeros(max_val)\n",
        "    prob_dist2 = np.zeros(max_val)\n",
        "\n",
        "    for val in data1:\n",
        "        prob_dist1[val] += 1\n",
        "    for val in data2:\n",
        "        prob_dist2[val] += 1\n",
        "\n",
        "    prob_dist1 /= np.sum(prob_dist1)\n",
        "    prob_dist2 /= np.sum(prob_dist2)\n",
        "\n",
        "    # Calculate JS divergence\n",
        "    js_divergence = jensenshannon(prob_dist1, prob_dist2, base=2)\n",
        "\n",
        "    return js_divergence\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# get tag mappings from the POS similarity scorer defined in `style'\n",
        "scorer = POSStyleSimilarityScorer()\n",
        "upos_mapper = {t:i for i,t in enumerate(scorer.VALID_UPOS)}\n",
        "\n",
        "# map texts to POS tag (IDs)\n",
        "original_tags, synthetic_tags = [], []\n",
        "for text in original_texts:\n",
        "    tags, _ = scorer.tag_and_filter(text)\n",
        "    original_tags.extend([upos_mapper[t] for t in tags])\n",
        "\n",
        "for text in synthetic_texts:\n",
        "    tags, _ = scorer.tag_and_filter(text)\n",
        "    synthetic_tags.extend([upos_mapper[t] for t in tags])\n",
        "\n",
        "print(f\"JS Divergence: {calculate_js_divergence(original_tags, synthetic_tags):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UefDCNltwB0Y",
        "outputId": "1bcd36d3-36db-4d4d-b720-fe3e62716eed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JS Divergence: 0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: compare POS trigrams at distribution level\n",
        "import numpy as np\n",
        "import spacy\n",
        "from nltk.util import trigrams\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def generate_pos_trigram_distribution(nlp, text):\n",
        "    tokens = []\n",
        "    for doc in nlp.pipe(\n",
        "        text,\n",
        "        disable=[\"ner\"]\n",
        "        ):\n",
        "        tokens.extend([token.pos_ for token in doc])\n",
        "    trigrams_generated = trigrams(tokens)\n",
        "    trigram_counts = Counter(trigrams_generated)\n",
        "    # normalize counts to create a distribution\n",
        "    total_count = sum(trigram_counts.values())\n",
        "    trigram_distribution = {trigram: count / total_count for trigram, count in trigram_counts.items()}\n",
        "\n",
        "    return trigram_distribution\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    kl_div = 0\n",
        "    for key in p:\n",
        "        p_val = p[key]\n",
        "        q_val = q.get(key, 0)  # default to 0 if key is not in q\n",
        "\n",
        "        # only consider non-zero p values\n",
        "        if p_val > 0:\n",
        "            if q_val > 0:\n",
        "                kl_div += p_val * np.log2(p_val / q_val)\n",
        "            else:\n",
        "                kl_div += p_val * np.log2(p_val / (q_val + 1e-10))  # avoid division by zero\n",
        "\n",
        "    return kl_div\n",
        "\n",
        "def js_divergence(distr1, distr2):\n",
        "    avg_distr = {k: (distr1.get(k, 0) + distr2.get(k, 0)) / 2 for k in set(distr1) | set(distr2)}\n",
        "    kl_div1 = kl_divergence(distr1, avg_distr)\n",
        "    kl_div2 = kl_divergence(distr2, avg_distr)\n",
        "    return (kl_div1 + kl_div2) / 2\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "trigram_distribution1 = generate_pos_trigram_distribution(nlp, original_texts)\n",
        "trigram_distribution2 = generate_pos_trigram_distribution(nlp, synthetic_texts)\n",
        "print(trigram_distribution1)\n",
        "print(trigram_distribution2)\n",
        "\n",
        "js_div = js_divergence(trigram_distribution1, trigram_distribution2)\n",
        "print(\"JS Divergence:\", js_div)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_WT2NR_aVJn",
        "outputId": "ede3d325-28c5-4ff7-bf72-1f6a4f2b1a68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('DET', 'ADJ', 'ADJ'): 0.10714285714285714, ('ADJ', 'ADJ', 'NOUN'): 0.10714285714285714, ('ADJ', 'NOUN', 'VERB'): 0.10714285714285714, ('NOUN', 'VERB', 'ADP'): 0.10714285714285714, ('VERB', 'ADP', 'DET'): 0.10714285714285714, ('ADP', 'DET', 'ADJ'): 0.10714285714285714, ('DET', 'ADJ', 'NOUN'): 0.10714285714285714, ('ADJ', 'NOUN', 'PUNCT'): 0.10714285714285714, ('NOUN', 'PUNCT', 'DET'): 0.07142857142857142, ('PUNCT', 'DET', 'ADJ'): 0.07142857142857142}\n",
            "{('DET', 'ADJ', 'ADJ'): 0.02564102564102564, ('ADJ', 'ADJ', 'NOUN'): 0.02564102564102564, ('ADJ', 'NOUN', 'VERB'): 0.02564102564102564, ('NOUN', 'VERB', 'ADP'): 0.02564102564102564, ('VERB', 'ADP', 'DET'): 0.02564102564102564, ('ADP', 'DET', 'ADJ'): 0.05128205128205128, ('DET', 'ADJ', 'NOUN'): 0.05128205128205128, ('ADJ', 'NOUN', 'PUNCT'): 0.05128205128205128, ('NOUN', 'PUNCT', 'DET'): 0.02564102564102564, ('PUNCT', 'DET', 'ADJ'): 0.02564102564102564, ('ADJ', 'NOUN', 'AUX'): 0.02564102564102564, ('NOUN', 'AUX', 'VERB'): 0.02564102564102564, ('AUX', 'VERB', 'ADV'): 0.02564102564102564, ('VERB', 'ADV', 'SCONJ'): 0.02564102564102564, ('ADV', 'SCONJ', 'PRON'): 0.02564102564102564, ('SCONJ', 'PRON', 'AUX'): 0.02564102564102564, ('PRON', 'AUX', 'VERB'): 0.02564102564102564, ('AUX', 'VERB', 'ADP'): 0.02564102564102564, ('VERB', 'ADP', 'ADP'): 0.02564102564102564, ('ADP', 'ADP', 'DET'): 0.02564102564102564, ('DET', 'ADJ', 'PUNCT'): 0.02564102564102564, ('ADJ', 'PUNCT', 'VERB'): 0.02564102564102564, ('PUNCT', 'VERB', 'ADJ'): 0.02564102564102564, ('VERB', 'ADJ', 'NOUN'): 0.02564102564102564, ('NOUN', 'PUNCT', 'SCONJ'): 0.02564102564102564, ('PUNCT', 'SCONJ', 'ADJ'): 0.02564102564102564, ('SCONJ', 'ADJ', 'NOUN'): 0.02564102564102564, ('ADJ', 'NOUN', 'NOUN'): 0.02564102564102564, ('NOUN', 'NOUN', 'NOUN'): 0.02564102564102564, ('NOUN', 'NOUN', 'PUNCT'): 0.02564102564102564, ('NOUN', 'PUNCT', 'VERB'): 0.02564102564102564, ('PUNCT', 'VERB', 'PART'): 0.02564102564102564, ('VERB', 'PART', 'VERB'): 0.02564102564102564, ('PART', 'VERB', 'DET'): 0.02564102564102564, ('VERB', 'DET', 'NOUN'): 0.02564102564102564, ('DET', 'NOUN', 'PUNCT'): 0.02564102564102564}\n",
            "JS Divergence: 0.46828573192759126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: compute divergence of character n-grams\n",
        "import json\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def generate_n_grams(texts, n, pad_token='|'):\n",
        "    \"\"\"Generate n-grams from the given list of texts, padding the end if necessary.\"\"\"\n",
        "    all_n_grams = []\n",
        "    for text in texts:\n",
        "        # determine the padding required to complete the last n-gram\n",
        "        padding_required = (n - len(text) % n) % n\n",
        "        padded_text = text + pad_token * padding_required\n",
        "        # generate n-grams from the padded text\n",
        "        n_grams = [padded_text[i:i+n] for i in range(0, len(padded_text), n)]\n",
        "        all_n_grams.extend(n_grams)\n",
        "    return all_n_grams\n",
        "\n",
        "def update_mapping(n_grams, mapping):\n",
        "    max_value = max(mapping.values(), default=0)\n",
        "    for n_gram in n_grams:\n",
        "        if n_gram not in mapping:\n",
        "            max_value += 1\n",
        "            mapping[n_gram] = max_value\n",
        "    return mapping\n",
        "\n",
        "def process_texts(texts, n=3, mapping={}):\n",
        "    n_grams = generate_n_grams(texts, n)\n",
        "    mapping = update_mapping(n_grams, mapping)\n",
        "    processed_texts = [[mapping[n_gram] for n_gram in generate_n_grams([text], n)] for text in texts]\n",
        "    return processed_texts, mapping\n",
        "\n",
        "def compute_freq_dist(mapping, processed_texts):\n",
        "    \"\"\"Compute frequency distribution of n-grams.\"\"\"\n",
        "    freq_dist = np.zeros(max(mapping.values()) + 1)\n",
        "    for text in processed_texts:\n",
        "        for n_gram_idx in text:\n",
        "            freq_dist[n_gram_idx] += 1\n",
        "    return freq_dist\n",
        "\n",
        "def normalize_dist(freq_dist):\n",
        "    \"\"\"Convert frequency distribution to probability distribution.\"\"\"\n",
        "    total_count = np.sum(freq_dist)\n",
        "    return freq_dist / total_count if total_count > 0 else np.zeros_like(freq_dist)\n",
        "\n",
        "def js_divergence(p, q):\n",
        "    m = 0.5 * (p + q)\n",
        "    p = np.where(p == 0, 1e-10, p)  # avoid log(0)\n",
        "    q = np.where(q == 0, 1e-10, q)\n",
        "    m = np.where(m == 0, 1e-10, m)\n",
        "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
        "\n",
        "processed_texts_original, ngram_to_id = process_texts(original_texts)\n",
        "processed_texts_synthetic, ngram_to_id = process_texts(synthetic_texts, mapping=ngram_to_id)\n",
        "freq_dist_original = compute_freq_dist(ngram_to_id, processed_texts_original)\n",
        "freq_dist_synthetic = compute_freq_dist(ngram_to_id, processed_texts_synthetic)\n",
        "\n",
        "# Convert to probability distributions and ensure they have the same length\n",
        "prob_dist_original = normalize_dist(freq_dist_original)\n",
        "prob_dist_synthetic = normalize_dist(freq_dist_synthetic)\n",
        "length = max(len(prob_dist_original), len(prob_dist_synthetic))\n",
        "prob_dist_original = np.pad(prob_dist_original, (0, length - len(prob_dist_original)), 'constant')\n",
        "prob_dist_synthetic = np.pad(prob_dist_synthetic, (0, length - len(prob_dist_synthetic)), 'constant')\n",
        "\n",
        "js_div = js_divergence(prob_dist_original, prob_dist_synthetic)\n",
        "\n",
        "print(f\"JS Divergence: {js_div:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8DdannubGe",
        "outputId": "08b7df54-ab96-4344-bab1-d5b4007fa46c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JS Divergence: 0.67\n"
          ]
        }
      ]
    }
  ]
}